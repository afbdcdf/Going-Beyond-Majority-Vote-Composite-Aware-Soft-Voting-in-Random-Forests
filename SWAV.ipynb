{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00GgnvfZOnP3",
        "outputId": "fbc916c9-d686-41d7-9c6e-55dc69cfcf09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-detected target column: Personality\n",
            "\n",
            "Standard Random Forest Performance:\n",
            "{\n",
            "  \"Extrovert\": {\n",
            "    \"precision\": 0.9143835616438356,\n",
            "    \"recall\": 0.8959731543624161,\n",
            "    \"f1-score\": 0.9050847457627119,\n",
            "    \"support\": 298.0\n",
            "  },\n",
            "  \"Introvert\": {\n",
            "    \"precision\": 0.8923611111111112,\n",
            "    \"recall\": 0.9113475177304965,\n",
            "    \"f1-score\": 0.9017543859649123,\n",
            "    \"support\": 282.0\n",
            "  },\n",
            "  \"accuracy\": 0.903448275862069,\n",
            "  \"macro avg\": {\n",
            "    \"precision\": 0.9033723363774734,\n",
            "    \"recall\": 0.9036603360464563,\n",
            "    \"f1-score\": 0.9034195658638121,\n",
            "    \"support\": 580.0\n",
            "  },\n",
            "  \"weighted avg\": {\n",
            "    \"precision\": 0.9036760943158558,\n",
            "    \"recall\": 0.903448275862069,\n",
            "    \"f1-score\": 0.9034655018610231,\n",
            "    \"support\": 580.0\n",
            "  }\n",
            "}\n",
            "\n",
            "Standard RF Log Loss: 0.6166\n",
            "\n",
            "Composite-Metric Softmax RF Performance:\n",
            "{\n",
            "  \"Extrovert\": {\n",
            "    \"precision\": 0.9140893470790378,\n",
            "    \"recall\": 0.8926174496644296,\n",
            "    \"f1-score\": 0.9032258064516129,\n",
            "    \"support\": 298.0\n",
            "  },\n",
            "  \"Introvert\": {\n",
            "    \"precision\": 0.889273356401384,\n",
            "    \"recall\": 0.9113475177304965,\n",
            "    \"f1-score\": 0.9001751313485113,\n",
            "    \"support\": 282.0\n",
            "  },\n",
            "  \"accuracy\": 0.9017241379310345,\n",
            "  \"macro avg\": {\n",
            "    \"precision\": 0.9016813517402109,\n",
            "    \"recall\": 0.901982483697463,\n",
            "    \"f1-score\": 0.9017004689000621,\n",
            "    \"support\": 580.0\n",
            "  },\n",
            "  \"weighted avg\": {\n",
            "    \"precision\": 0.9020236412667992,\n",
            "    \"recall\": 0.9017241379310345,\n",
            "    \"f1-score\": 0.9017425471773464,\n",
            "    \"support\": 580.0\n",
            "  }\n",
            "}\n",
            "\n",
            "Composite Softmax RF Log Loss: 0.6169\n",
            "\n",
            "Composite Scores (first 10 trees): [3.6014 3.6071 3.6277 3.621  3.6214 3.5795 3.5725 3.5793 3.5793 3.6072]\n",
            "Softmax Weights   (first 10 trees): [0.0103 0.0104 0.0106 0.0105 0.0105 0.0101 0.01   0.0101 0.0101 0.0104]\n",
            "\n",
            "Standard Random Forest Summary:\n",
            "  Accuracy : 0.9034\n",
            "  Precision: 0.9037\n",
            "  Recall   : 0.9034\n",
            "  F1-score : 0.9035\n",
            "  Log-loss : 0.6166\n",
            "\n",
            "Composite-Metric RF (softmax) Summary:\n",
            "  Accuracy : 0.9017\n",
            "  Precision: 0.9020\n",
            "  Recall   : 0.9017\n",
            "  F1-score : 0.9017\n",
            "  Log-loss : 0.6169\n",
            "\n",
            "Stacked MOE Model Performance:\n",
            "{\n",
            "  \"Extrovert\": {\n",
            "    \"precision\": 0.9143835616438356,\n",
            "    \"recall\": 0.8959731543624161,\n",
            "    \"f1-score\": 0.9050847457627119,\n",
            "    \"support\": 298.0\n",
            "  },\n",
            "  \"Introvert\": {\n",
            "    \"precision\": 0.8923611111111112,\n",
            "    \"recall\": 0.9113475177304965,\n",
            "    \"f1-score\": 0.9017543859649123,\n",
            "    \"support\": 282.0\n",
            "  },\n",
            "  \"accuracy\": 0.903448275862069,\n",
            "  \"macro avg\": {\n",
            "    \"precision\": 0.9033723363774734,\n",
            "    \"recall\": 0.9036603360464563,\n",
            "    \"f1-score\": 0.9034195658638121,\n",
            "    \"support\": 580.0\n",
            "  },\n",
            "  \"weighted avg\": {\n",
            "    \"precision\": 0.9036760943158558,\n",
            "    \"recall\": 0.903448275862069,\n",
            "    \"f1-score\": 0.9034655018610231,\n",
            "    \"support\": 580.0\n",
            "  }\n",
            "}\n",
            "\n",
            "Stacked MOE Model Log Loss: 0.2836\n",
            "\n",
            "\n",
            "Stacked MOE Model Summary:\n",
            "  Accuracy : 0.9034\n",
            "  Precision: 0.9037\n",
            "  Recall   : 0.9034\n",
            "  F1-score : 0.9035\n",
            "  Log-loss : 0.2836\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, log_loss\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.special import softmax\n",
        "import json\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "#spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "# === User configuration ===\n",
        "FILE_PATH = \"/content/personality_dataset.csv\"  # Path to your data file (.csv or .data)\n",
        "TARGET_COLUMN = 'Personality'          # Name of the target column, or None to auto-detect\n",
        "TEST_SIZE = 0.2                    # Fraction of data to reserve for testing\n",
        "VAL_SIZE = 0.25                    # Fraction of train+val data to reserve for validation\n",
        "N_ESTIMATORS = 100                 # Number of trees in each Random Forest\n",
        "RANDOM_STATE = 42                  # Random seed for reproducibility\n",
        "TEMPERATURE = 1.0                  # Softmax temperature (<1 -> sharper; >1 -> smoother)\n",
        "COMPOSITE_WEIGHTS = [1.0, 1.0, 1.0, 1.0]  # [accuracy, precision, recall, f1]\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dataset by file extension. Supports .csv and .data as CSVs.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext in ['.csv', '.data']:\n",
        "        return pd.read_csv(file_path)\n",
        "    raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "\n",
        "def detect_target_column(df):\n",
        "    \"\"\"\n",
        "    Auto-detect target column: prefers 'target', then 'class', otherwise last column.\n",
        "    \"\"\"\n",
        "    if 'target' in df.columns:\n",
        "        return 'target'\n",
        "    if 'class' in df.columns:\n",
        "        return 'class'\n",
        "    return df.columns[-1]\n",
        "\n",
        "\n",
        "def evaluate_random_forests(df, target_column,\n",
        "                            test_size=TEST_SIZE, val_size=VAL_SIZE,\n",
        "                            n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE,\n",
        "                            temperature=TEMPERATURE, composite_weights=COMPOSITE_WEIGHTS):\n",
        "    \"\"\"\n",
        "    Trains one Random Forest and evaluates:\n",
        "      - Standard RF with majority voting\n",
        "      - Composite-metric weighted RF using multiple metrics per tree\n",
        "    Returns metrics for RF, plus train/test splits, label encoder, and trained RF.\n",
        "    \"\"\"\n",
        "    # Prepare features and labels\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Identify categorical columns for one-hot encoding\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "    numerical_cols = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Create a preprocessor using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', 'passthrough', numerical_cols),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "        ],\n",
        "        remainder='passthrough' # Keep other columns (if any)\n",
        "    )\n",
        "\n",
        "    # Encode string labels to integers\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "\n",
        "    # Split into train+val and test\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y_enc, test_size=test_size, random_state=random_state, stratify=y_enc\n",
        "    )\n",
        "    # Split train and val\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=val_size, random_state=random_state, stratify=y_train_val\n",
        "    )\n",
        "\n",
        "    # Apply preprocessing\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_val_processed = preprocessor.transform(X_val)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "    # Train a single Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
        "    rf.fit(X_train_processed, y_train)\n",
        "\n",
        "    # --- Standard evaluation ---\n",
        "    proba_std = rf.predict_proba(X_test_processed)\n",
        "    log_std = log_loss(y_test, proba_std)\n",
        "    y_pred_std = np.argmax(proba_std, axis=1)\n",
        "    metrics_standard = classification_report(\n",
        "        le.inverse_transform(y_test),\n",
        "        le.inverse_transform(y_pred_std),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    # --- Composite-metric weighting ---\n",
        "    # Compute per-tree metrics on validation set\n",
        "    tree_acc, tree_prec, tree_rec, tree_f1 = [], [], [], []\n",
        "    for tree in rf.estimators_:\n",
        "        # Need to pass processed data to the tree\n",
        "        preds_val = tree.predict(X_val_processed)\n",
        "        tree_acc.append(accuracy_score(y_val, preds_val))\n",
        "        tree_prec.append(precision_score(y_val, preds_val, average='weighted', zero_division=0))\n",
        "        tree_rec.append(recall_score(y_val, preds_val, average='weighted', zero_division=0))\n",
        "        tree_f1.append(f1_score(y_val, preds_val, average='weighted', zero_division=0))\n",
        "    metrics_matrix = np.vstack([tree_acc, tree_prec, tree_rec, tree_f1]).T\n",
        "    composite_scores = metrics_matrix.dot(np.array(composite_weights))\n",
        "    weights = softmax(composite_scores / temperature)\n",
        "\n",
        "    # Aggregate weighted probabilities\n",
        "    proba_weighted = np.zeros_like(proba_std)\n",
        "    for w, tree in zip(weights, rf.estimators_):\n",
        "        # Need to pass processed data to the tree\n",
        "        proba_weighted += w * tree.predict_proba(X_test_processed)\n",
        "    log_weighted = log_loss(y_test, proba_weighted)\n",
        "    y_pred_w = np.argmax(proba_weighted, axis=1)\n",
        "    metrics_weighted = classification_report(\n",
        "        le.inverse_transform(y_test),\n",
        "        le.inverse_transform(y_pred_w),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    return (metrics_standard, metrics_weighted, log_std, log_weighted,\n",
        "            composite_scores, weights, X_train_processed, y_train, X_test_processed, y_test, le, rf, preprocessor)\n",
        "\n",
        "\n",
        "def print_summary(metrics, title, logloss=None):\n",
        "    \"\"\"\n",
        "    Prints overall accuracy, weighted-average precision, recall, F1, and optional log-loss.\n",
        "    \"\"\"\n",
        "    accuracy = metrics.get('accuracy')\n",
        "    weighted_avg = metrics.get('weighted avg', {})\n",
        "    precision = weighted_avg.get('precision')\n",
        "    recall = weighted_avg.get('recall')\n",
        "    f1 = weighted_avg.get('f1-score')\n",
        "\n",
        "    print(f\"\\n{title} Summary:\")\n",
        "    if accuracy is not None:\n",
        "        print(f\"  Accuracy : {accuracy:.4f}\")\n",
        "    if precision is not None:\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "    if recall is not None:\n",
        "        print(f\"  Recall   : {recall:.4f}\")\n",
        "    if f1 is not None:\n",
        "        print(f\"  F1-score : {f1:.4f}\")\n",
        "    if logloss is not None:\n",
        "        print(f\"  Log-loss : {logloss:.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    df = load_data(FILE_PATH)\n",
        "    target_col = TARGET_COLUMN or detect_target_column(df)\n",
        "    print(f\"Auto-detected target column: {target_col}\\n\")\n",
        "\n",
        "    # Evaluate Random Forests and get splits\n",
        "    (metrics_standard, metrics_weighted,\n",
        "     log_std, log_weighted,\n",
        "     composite_scores, weights,\n",
        "     X_train_processed, y_train, X_test_processed, y_test,\n",
        "     le, rf, preprocessor) = evaluate_random_forests(df, target_col)\n",
        "\n",
        "    # Full reports for RF models\n",
        "    print(\"Standard Random Forest Performance:\")\n",
        "    print(json.dumps(metrics_standard, indent=2))\n",
        "    print(f\"\\nStandard RF Log Loss: {log_std:.4f}\\n\")\n",
        "\n",
        "    print(\"Composite-Metric Softmax RF Performance:\")\n",
        "    print(json.dumps(metrics_weighted, indent=2))\n",
        "    print(f\"\\nComposite Softmax RF Log Loss: {log_weighted:.4f}\\n\")\n",
        "\n",
        "    # Display composite scores and weight distribution\n",
        "    print(f\"Composite Scores (first 10 trees): {np.round(composite_scores[:10], 4)}\")\n",
        "    print(f\"Softmax Weights   (first 10 trees): {np.round(weights[:10], 4)}\")\n",
        "\n",
        "    # Concise summaries including log-loss\n",
        "    print_summary(metrics_standard, \"Standard Random Forest\", log_std)\n",
        "    print_summary(metrics_weighted, \"Composite-Metric RF (softmax)\", log_weighted)\n",
        "\n",
        "    # === Stacked MOE Model Evaluation ===\n",
        "    # Define a simple stacking (MOE) classifier using RF as an expert\n",
        "    experts = [('rf', rf)]\n",
        "    stacked_moe = StackingClassifier(\n",
        "        estimators=experts,\n",
        "        final_estimator=LogisticRegression(),\n",
        "        cv=5,\n",
        "        stack_method='predict_proba',\n",
        "        passthrough=False,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    stacked_moe.fit(X_train_processed, y_train)\n",
        "\n",
        "    # Evaluate the Stacked MOE on the test set\n",
        "    proba_moe = stacked_moe.predict_proba(X_test_processed)\n",
        "    log_moe = log_loss(y_test, proba_moe)\n",
        "    y_pred_moe = np.argmax(proba_moe, axis=1)\n",
        "\n",
        "    metrics_moe = classification_report(\n",
        "        le.inverse_transform(y_test),\n",
        "        le.inverse_transform(y_pred_moe),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nStacked MOE Model Performance:\")\n",
        "    print(json.dumps(metrics_moe, indent=2))\n",
        "    print(f\"\\nStacked MOE Model Log Loss: {log_moe:.4f}\\n\")\n",
        "    print_summary(metrics_moe, \"Stacked MOE Model\", log_moe)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dw abt this"
      ],
      "metadata": {
        "id": "EB385HRdngRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, log_loss\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from scipy.special import softmax\n",
        "import json\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# === User configuration ===\n",
        "TRAIN_PATH = \"/content/train.csv\"  # Path to your training data file\n",
        "TEST_PATH  = \"/content/test.csv\"   # Path to your test data file\n",
        "TARGET_COLUMN = 'satisfaction'                    # Name of the target column, or None to auto-detect\n",
        "VAL_SIZE = 0.25                            # Fraction of TRAIN data to reserve for validation\n",
        "N_ESTIMATORS = 100                          # Number of trees in each Random Forest\n",
        "RANDOM_STATE = 42                          # Random seed for reproducibility\n",
        "TEMPERATURE = 1.0                          # Softmax temperature (<1 -> sharper; >1 -> smoother)\n",
        "COMPOSITE_WEIGHTS = [1.0, 1.0, 1.0, 1.0]    # [accuracy, precision, recall, f1]\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dataset by file extension. Supports .csv and .data as CSVs.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext in ['.csv', '.data']:\n",
        "        return pd.read_csv(file_path)\n",
        "    raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "\n",
        "def detect_target_column(df):\n",
        "    \"\"\"\n",
        "    Auto-detect target column: prefers 'target', then 'class', otherwise last column.\n",
        "    \"\"\"\n",
        "    if 'target' in df.columns:\n",
        "        return 'target'\n",
        "    if 'class' in df.columns:\n",
        "        return 'class'\n",
        "    return df.columns[-1]\n",
        "\n",
        "\n",
        "def evaluate_random_forests_explicit(df_train, df_test, target_column,\n",
        "                                    val_size=VAL_SIZE, n_estimators=N_ESTIMATORS,\n",
        "                                    random_state=RANDOM_STATE, temperature=TEMPERATURE,\n",
        "                                    composite_weights=COMPOSITE_WEIGHTS):\n",
        "    \"\"\"\n",
        "    Trains one Random Forest on df_train (optionally splitting off validation),\n",
        "    evaluates on df_test, for both standard majority-vote RF and composite-metric softmax RF.\n",
        "    Returns metrics, log-losses, composite scores, weights, processed splits, label encoder, RF, and preprocessor.\n",
        "    \"\"\"\n",
        "    # Split out features & labels\n",
        "    X_train_full = df_train.drop(columns=[target_column])\n",
        "    y_train_full = df_train[target_column]\n",
        "    X_test = df_test.drop(columns=[target_column])\n",
        "    y_test = df_test[target_column]\n",
        "\n",
        "    # Encode labels to integers\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train_full)\n",
        "    y_test_enc  = le.transform(y_test)\n",
        "\n",
        "    # Optionally split train into train + validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full, y_train_enc, test_size=val_size,\n",
        "        random_state=random_state, stratify=y_train_enc\n",
        "    )\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "    numerical_cols   = X_train.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Build preprocessing pipeline\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', 'passthrough', numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "    # Fit on train, transform train/val/test\n",
        "    X_train_proc = preprocessor.fit_transform(X_train)\n",
        "    X_val_proc   = preprocessor.transform(X_val)\n",
        "    X_test_proc  = preprocessor.transform(X_test)\n",
        "\n",
        "    # Train a single Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
        "    rf.fit(X_train_proc, y_train)\n",
        "\n",
        "    # --- Standard RF evaluation ---\n",
        "    proba_std = rf.predict_proba(X_test_proc)\n",
        "    log_std = log_loss(y_test_enc, proba_std)\n",
        "    y_pred_std = np.argmax(proba_std, axis=1)\n",
        "    metrics_standard = classification_report(\n",
        "        le.inverse_transform(y_test_enc),\n",
        "        le.inverse_transform(y_pred_std),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    # --- Composite-metric softmax RF ---\n",
        "    # Compute per-tree metrics on validation set\n",
        "    tree_acc, tree_prec, tree_rec, tree_f1 = [], [], [], []\n",
        "    for tree in rf.estimators_:\n",
        "        preds_val = tree.predict(X_val_proc)\n",
        "        tree_acc.append(accuracy_score(y_val, preds_val))\n",
        "        tree_prec.append(precision_score(y_val, preds_val, average='weighted', zero_division=0))\n",
        "        tree_rec.append(recall_score(y_val, preds_val, average='weighted', zero_division=0))\n",
        "        tree_f1.append(f1_score(y_val, preds_val, average='weighted', zero_division=0))\n",
        "\n",
        "    metrics_matrix = np.vstack([tree_acc, tree_prec, tree_rec, tree_f1]).T\n",
        "    composite_scores = metrics_matrix.dot(np.array(composite_weights))\n",
        "    weights = softmax(composite_scores / temperature)\n",
        "\n",
        "    # Aggregate weighted probabilities\n",
        "    proba_weighted = np.zeros_like(proba_std)\n",
        "    for w, tree in zip(weights, rf.estimators_):\n",
        "        proba_weighted += w * tree.predict_proba(X_test_proc)\n",
        "\n",
        "    log_weighted = log_loss(y_test_enc, proba_weighted)\n",
        "    y_pred_w = np.argmax(proba_weighted, axis=1)\n",
        "    metrics_weighted = classification_report(\n",
        "        le.inverse_transform(y_test_enc),\n",
        "        le.inverse_transform(y_pred_w),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    return (metrics_standard, metrics_weighted,\n",
        "            log_std, log_weighted,\n",
        "            composite_scores, weights,\n",
        "            X_train_proc, y_train, X_test_proc, y_test_enc,\n",
        "            le, rf, preprocessor)\n",
        "\n",
        "\n",
        "def print_summary(metrics, title, logloss=None):\n",
        "    \"\"\"\n",
        "    Prints overall accuracy, weighted-average precision, recall, F1, and optional log-loss.\n",
        "    \"\"\"\n",
        "    accuracy = metrics.get('accuracy')\n",
        "    weighted = metrics.get('weighted avg', {})\n",
        "    precision = weighted.get('precision')\n",
        "    recall = weighted.get('recall')\n",
        "    f1 = weighted.get('f1-score')\n",
        "\n",
        "    print(f\"\\n{title} Summary:\")\n",
        "    if accuracy is not None:\n",
        "        print(f\"  Accuracy : {accuracy:.4f}\")\n",
        "    if precision is not None:\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "    if recall is not None:\n",
        "        print(f\"  Recall   : {recall:.4f}\")\n",
        "    if f1 is not None:\n",
        "        print(f\"  F1-score : {f1:.4f}\")\n",
        "    if logloss is not None:\n",
        "        print(f\"  Log-loss : {logloss:.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load train and test files\n",
        "    df_train = load_data(TRAIN_PATH)\n",
        "    df_test  = load_data(TEST_PATH)\n",
        "\n",
        "    # Determine target column\n",
        "    target_col = TARGET_COLUMN or detect_target_column(df_train)\n",
        "    print(f\"Using '{target_col}' as target column.\\n\")\n",
        "\n",
        "    # Evaluate Random Forests with explicit train/test\n",
        "    (metrics_std, metrics_w,\n",
        "     log_std, log_w,\n",
        "     comp_scores, weights,\n",
        "     X_train_p, y_train, X_test_p, y_test_enc,\n",
        "     le, rf, preprocessor) = evaluate_random_forests_explicit(\n",
        "         df_train, df_test, target_col\n",
        "    )\n",
        "\n",
        "    # Report standard RF\n",
        "    print(\"Standard Random Forest Performance:\")\n",
        "    print(json.dumps(metrics_std, indent=2))\n",
        "    print(f\"\\nStandard RF Log Loss: {log_std:.4f}\")\n",
        "    print_summary(metrics_std, \"Standard RF\", log_std)\n",
        "\n",
        "    # Report composite-metric RF\n",
        "    print(\"\\nComposite-Metric Softmax RF Performance:\")\n",
        "    print(json.dumps(metrics_w, indent=2))\n",
        "    print(f\"\\nComposite Softmax RF Log Loss: {log_w:.4f}\")\n",
        "    print_summary(metrics_w, \"Composite-Metric RF (softmax)\", log_w)\n",
        "\n",
        "    # Show example composite scores & weights\n",
        "    print(f\"\\nComposite Scores (first 10 trees): {np.round(comp_scores[:10],4)}\")\n",
        "    print(f\"Weights            (first 10 trees): {np.round(weights[:10],4)}\")\n",
        "\n",
        "    # === Stacked MOE Model Evaluation ===\n",
        "    experts = [('rf', rf)]\n",
        "    stacked_moe = StackingClassifier(\n",
        "        estimators=experts,\n",
        "        final_estimator=LogisticRegression(),\n",
        "        cv=5,\n",
        "        stack_method='predict_proba',\n",
        "        passthrough=False,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    stacked_moe.fit(X_train_p, y_train)\n",
        "\n",
        "    proba_moe = stacked_moe.predict_proba(X_test_p)\n",
        "    log_moe = log_loss(y_test_enc, proba_moe)\n",
        "    y_pred_moe = np.argmax(proba_moe, axis=1)\n",
        "    metrics_moe = classification_report(\n",
        "        le.inverse_transform(y_test_enc),\n",
        "        le.inverse_transform(y_pred_moe),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nStacked MOE Model Performance:\")\n",
        "    print(json.dumps(metrics_moe, indent=2))\n",
        "    print(f\"\\nStacked MOE Model Log Loss: {log_moe:.4f}\")\n",
        "    print_summary(metrics_moe, \"Stacked MOE Model\", log_moe)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZvD7G3fBnfXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128738d0-a9f5-4933-e4c6-91505ca9f4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 'satisfaction' as target column.\n",
            "\n",
            "Standard Random Forest Performance:\n",
            "{\n",
            "  \"neutral or dissatisfied\": {\n",
            "    \"precision\": 0.957259796027912,\n",
            "    \"recall\": 0.9790022644616757,\n",
            "    \"f1-score\": 0.9680089561352919,\n",
            "    \"support\": 14573.0\n",
            "  },\n",
            "  \"satisfied\": {\n",
            "    \"precision\": 0.9723627167630058,\n",
            "    \"recall\": 0.9441375076734193,\n",
            "    \"f1-score\": 0.9580422691879866,\n",
            "    \"support\": 11403.0\n",
            "  },\n",
            "  \"accuracy\": 0.9636972590083154,\n",
            "  \"macro avg\": {\n",
            "    \"precision\": 0.964811256395459,\n",
            "    \"recall\": 0.9615698860675475,\n",
            "    \"f1-score\": 0.9630256126616392,\n",
            "    \"support\": 25976.0\n",
            "  },\n",
            "  \"weighted avg\": {\n",
            "    \"precision\": 0.9638897084525453,\n",
            "    \"recall\": 0.9636972590083154,\n",
            "    \"f1-score\": 0.9636337585967902,\n",
            "    \"support\": 25976.0\n",
            "  }\n",
            "}\n",
            "\n",
            "Standard RF Log Loss: 0.1079\n",
            "\n",
            "Standard RF Summary:\n",
            "  Accuracy : 0.9637\n",
            "  Precision: 0.9639\n",
            "  Recall   : 0.9637\n",
            "  F1-score : 0.9636\n",
            "  Log-loss : 0.1079\n",
            "\n",
            "Composite-Metric Softmax RF Performance:\n",
            "{\n",
            "  \"neutral or dissatisfied\": {\n",
            "    \"precision\": 0.9581317204301075,\n",
            "    \"recall\": 0.9783160639538874,\n",
            "    \"f1-score\": 0.9681186975859845,\n",
            "    \"support\": 14573.0\n",
            "  },\n",
            "  \"satisfied\": {\n",
            "    \"precision\": 0.971521268925739,\n",
            "    \"recall\": 0.94536525475752,\n",
            "    \"f1-score\": 0.958264811769412,\n",
            "    \"support\": 11403.0\n",
            "  },\n",
            "  \"accuracy\": 0.9638512473052048,\n",
            "  \"macro avg\": {\n",
            "    \"precision\": 0.9648264946779233,\n",
            "    \"recall\": 0.9618406593557036,\n",
            "    \"f1-score\": 0.9631917546776982,\n",
            "    \"support\": 25976.0\n",
            "  },\n",
            "  \"weighted avg\": {\n",
            "    \"precision\": 0.9640094930469725,\n",
            "    \"recall\": 0.9638512473052048,\n",
            "    \"f1-score\": 0.9637930177289482,\n",
            "    \"support\": 25976.0\n",
            "  }\n",
            "}\n",
            "\n",
            "Composite Softmax RF Log Loss: 0.1078\n",
            "\n",
            "Composite-Metric RF (softmax) Summary:\n",
            "  Accuracy : 0.9639\n",
            "  Precision: 0.9640\n",
            "  Recall   : 0.9639\n",
            "  F1-score : 0.9638\n",
            "  Log-loss : 0.1078\n",
            "\n",
            "Composite Scores (first 10 trees): [3.7095 3.6771 3.6906 3.6888 3.707  3.718  3.6789 3.6661 3.7148 3.7138]\n",
            "Weights            (first 10 trees): [0.0101 0.0098 0.0099 0.0099 0.01   0.0102 0.0098 0.0096 0.0101 0.0101]\n",
            "\n",
            "Stacked MOE Model Performance:\n",
            "{\n",
            "  \"neutral or dissatisfied\": {\n",
            "    \"precision\": 0.9610644637053087,\n",
            "    \"recall\": 0.9739243807040417,\n",
            "    \"f1-score\": 0.9674516887631641,\n",
            "    \"support\": 14573.0\n",
            "  },\n",
            "  \"satisfied\": {\n",
            "    \"precision\": 0.9660956459671663,\n",
            "    \"recall\": 0.9495746733315794,\n",
            "    \"f1-score\": 0.9577639202158241,\n",
            "    \"support\": 11403.0\n",
            "  },\n",
            "  \"accuracy\": 0.9632352941176471,\n",
            "  \"macro avg\": {\n",
            "    \"precision\": 0.9635800548362375,\n",
            "    \"recall\": 0.9617495270178106,\n",
            "    \"f1-score\": 0.9626078044894941,\n",
            "    \"support\": 25976.0\n",
            "  },\n",
            "  \"weighted avg\": {\n",
            "    \"precision\": 0.9632730628480544,\n",
            "    \"recall\": 0.9632352941176471,\n",
            "    \"f1-score\": 0.9631989314200274,\n",
            "    \"support\": 25976.0\n",
            "  }\n",
            "}\n",
            "\n",
            "Stacked MOE Model Log Loss: 0.1012\n",
            "\n",
            "Stacked MOE Model Summary:\n",
            "  Accuracy : 0.9632\n",
            "  Precision: 0.9633\n",
            "  Recall   : 0.9632\n",
            "  F1-score : 0.9632\n",
            "  Log-loss : 0.1012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dw abt htis either"
      ],
      "metadata": {
        "id": "N2Py266HrvwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    log_loss\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from scipy.special import softmax\n",
        "import json\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Fetch example dataset (unused in this script but kept for context)\n",
        "spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "# === User configuration ===\n",
        "FILE_PATH = \"/content/defaultOfCreditCardClients.xls\"  # Path to your data file (.csv, .data, .xls, .xlsx)\n",
        "TARGET_COLUMN = 'Y'          # Name of the target column, or None to auto-detect\n",
        "TEST_SIZE = 0.2                    # Fraction of data to reserve for testing\n",
        "VAL_SIZE = 0.25                    # Fraction of train+val data to reserve for validation\n",
        "N_ESTIMATORS = 30                 # Number of trees in each Random Forest\n",
        "RANDOM_STATE = 42                  # Random seed for reproducibility\n",
        "TEMPERATURE = 1.0                  # Softmax temperature (<1 -> sharper; >1 -> smoother)\n",
        "COMPOSITE_WEIGHTS = [1.0, 1.0, 1.0, 1.0]  # [accuracy, precision, recall, f1]\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dataset by file extension. Supports:\n",
        "      - .csv / .data via read_csv\n",
        "      - .xls / .xlsx via read_excel (skipping 2nd row)\n",
        "    After loading, always drop the first column.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext in ['.csv', '.data']:\n",
        "        df = pd.read_csv(file_path)\n",
        "    elif ext in ['.xls', '.xlsx']:\n",
        "        # header=0 → first row as column names; skiprows=[1] → drop 2nd row\n",
        "        df = pd.read_excel(file_path, header=0, skiprows=[1])\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "    # Drop the first column (position 0) unconditionally\n",
        "    return df.drop(df.columns[0], axis=1)\n",
        "\n",
        "\n",
        "def detect_target_column(df):\n",
        "    \"\"\"\n",
        "    Auto-detect target column: prefers 'target', then 'class', otherwise last column.\n",
        "    \"\"\"\n",
        "    if 'target' in df.columns:\n",
        "        return 'target'\n",
        "    if 'class' in df.columns:\n",
        "        return 'class'\n",
        "    return df.columns[-1]\n",
        "\n",
        "\n",
        "def evaluate_random_forests(\n",
        "    df, target_column,\n",
        "    test_size=TEST_SIZE, val_size=VAL_SIZE,\n",
        "    n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE,\n",
        "    temperature=TEMPERATURE, composite_weights=COMPOSITE_WEIGHTS\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains one Random Forest and evaluates:\n",
        "      - Standard RF with majority voting\n",
        "      - Composite-metric weighted RF using multiple metrics per tree\n",
        "    Returns metrics for RF, plus train/test splits, label encoder, and trained RF.\n",
        "    \"\"\"\n",
        "    # Prepare features and labels\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "    numerical_cols   = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Build preprocessing pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', 'passthrough', numerical_cols),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Encode target labels\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "\n",
        "    # Split into train+val and test\n",
        "    X_tr_val, X_test, y_tr_val, y_test = train_test_split(\n",
        "        X, y_enc,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=y_enc\n",
        "    )\n",
        "    # Split train and validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_tr_val, y_tr_val,\n",
        "        test_size=val_size,\n",
        "        random_state=random_state,\n",
        "        stratify=y_tr_val\n",
        "    )\n",
        "\n",
        "    # Preprocess features\n",
        "    X_train_p = preprocessor.fit_transform(X_train)\n",
        "    X_val_p   = preprocessor.transform(X_val)\n",
        "    X_test_p  = preprocessor.transform(X_test)\n",
        "\n",
        "    # Train Random Forest\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    rf.fit(X_train_p, y_train)\n",
        "\n",
        "    # Standard RF evaluation\n",
        "    proba_std = rf.predict_proba(X_test_p)\n",
        "    log_std   = log_loss(y_test, proba_std)\n",
        "    y_pred_std = np.argmax(proba_std, axis=1)\n",
        "    metrics_std = classification_report(\n",
        "        le.inverse_transform(y_test),\n",
        "        le.inverse_transform(y_pred_std),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    # Composite-metric weighted RF\n",
        "    # Compute per-tree metrics on validation set\n",
        "    metrics_matrix = []\n",
        "    for tree in rf.estimators_:\n",
        "        preds_val = tree.predict(X_val_p)\n",
        "        acc  = accuracy_score(y_val, preds_val)\n",
        "        prec = precision_score(y_val, preds_val, average='weighted', zero_division=0)\n",
        "        rec  = recall_score(y_val, preds_val, average='weighted', zero_division=0)\n",
        "        f1   = f1_score(y_val, preds_val, average='weighted', zero_division=0)\n",
        "        metrics_matrix.append([acc, prec, rec, f1])\n",
        "    metrics_matrix = np.array(metrics_matrix)  # shape=(n_trees, 4)\n",
        "\n",
        "    # Composite scores and softmax weights\n",
        "    composite_scores = metrics_matrix.dot(np.array(composite_weights))\n",
        "    weights = softmax(composite_scores / temperature)\n",
        "\n",
        "    # Weighted probability aggregation\n",
        "    proba_w = np.zeros_like(proba_std)\n",
        "    for w, tree in zip(weights, rf.estimators_):\n",
        "        proba_w += w * tree.predict_proba(X_test_p)\n",
        "    log_w = log_loss(y_test, proba_w)\n",
        "    y_pred_w = np.argmax(proba_w, axis=1)\n",
        "    metrics_w = classification_report(\n",
        "        le.inverse_transform(y_test),\n",
        "        le.inverse_transform(y_pred_w),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        metrics_std, metrics_w,\n",
        "        log_std, log_w,\n",
        "        composite_scores, weights,\n",
        "        X_train_p, y_train,\n",
        "        X_test_p, y_test,\n",
        "        le, rf, preprocessor\n",
        "    )\n",
        "\n",
        "\n",
        "def print_summary(metrics, title, logloss=None):\n",
        "    \"\"\"\n",
        "    Prints overall accuracy, weighted-average precision, recall, F1, and optional log-loss.\n",
        "    \"\"\"\n",
        "    accuracy     = metrics.get('accuracy')\n",
        "    weighted_avg = metrics.get('weighted avg', {})\n",
        "    precision    = weighted_avg.get('precision')\n",
        "    recall       = weighted_avg.get('recall')\n",
        "    f1           = weighted_avg.get('f1-score')\n",
        "\n",
        "    print(f\"\\n{title} Summary:\")\n",
        "    if accuracy is not None:\n",
        "        print(f\"  Accuracy : {accuracy:.4f}\")\n",
        "    if precision is not None:\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "    if recall is not None:\n",
        "        print(f\"  Recall   : {recall:.4f}\")\n",
        "    if f1 is not None:\n",
        "        print(f\"  F1-score : {f1:.4f}\")\n",
        "    if logloss is not None:\n",
        "        print(f\"  Log-loss : {logloss:.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load data (skips 2nd row and drops 1st column for Excel)\n",
        "    df = load_data(FILE_PATH)\n",
        "    target_col = TARGET_COLUMN or detect_target_column(df)\n",
        "    print(f\"Auto-detected target column: {target_col}\\n\")\n",
        "\n",
        "    # Evaluate Random Forests\n",
        "    (\n",
        "        metrics_std, metrics_w,\n",
        "        log_std, log_w,\n",
        "        composite_scores, weights,\n",
        "        X_train_p, y_train,\n",
        "        X_test_p, y_test,\n",
        "        le, rf, preprocessor\n",
        "    ) = evaluate_random_forests(df, target_col)\n",
        "\n",
        "    # Print full reports\n",
        "    print(\"Standard Random Forest Performance:\")\n",
        "    print(json.dumps(metrics_std, indent=2))\n",
        "    print(f\"\\nStandard RF Log Loss: {log_std:.4f}\\n\")\n",
        "\n",
        "    print(\"Composite-Metric Softmax RF Performance:\")\n",
        "    print(json.dumps(metrics_w, indent=2))\n",
        "    print(f\"\\nComposite Softmax RF Log Loss: {log_w:.4f}\\n\")\n",
        "\n",
        "    # Display composite scores and weight distribution\n",
        "    print(f\"Composite Scores (first 10 trees): {np.round(composite_scores[:10], 4)}\")\n",
        "    print(f\"Softmax Weights   (first 10 trees): {np.round(weights[:10], 4)}\")\n",
        "\n",
        "    # Concise summaries\n",
        "    print_summary(metrics_std, \"Standard Random Forest\", log_std)\n",
        "    print_summary(metrics_w, \"Composite-Metric RF (softmax)\", log_w)\n",
        "\n",
        "    # === Stacked MOE Model Evaluation ===\n",
        "    experts = [('rf', rf)]\n",
        "    stacked_moe = StackingClassifier(\n",
        "        estimators=experts,\n",
        "        final_estimator=LogisticRegression(),\n",
        "        cv=5,\n",
        "        stack_method='predict_proba',\n",
        "        passthrough=False,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    stacked_moe.fit(X_train_p, y_train)\n",
        "\n",
        "    proba_moe = stacked_moe.predict_proba(X_test_p)\n",
        "    log_moe   = log_loss(y_test, proba_moe)\n",
        "    y_pred_moe = np.argmax(proba_moe, axis=1)\n",
        "    metrics_moe = classification_report(\n",
        "        le.inverse_transform(y_test),\n",
        "        le.inverse_transform(y_pred_moe),\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nStacked MOE Model Performance:\")\n",
        "    print(json.dumps(metrics_moe, indent=2))\n",
        "    print(f\"\\nStacked MOE Model Log Loss: {log_moe:.4f}\\n\")\n",
        "    print_summary(metrics_moe, \"Stacked MOE Model\", log_moe)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZEDNoT0xrxRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}